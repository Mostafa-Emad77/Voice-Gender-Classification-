{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Gender Classification Pipeline\n",
    "\n",
    "*This is the second notebook out of three notebooks focused on finetuning the Wav2Vec Model.*\n",
    "\n",
    "This pipeline walks through downloading, extracting, processing audio files, training a Wav2Vec2 model for gender classification, and testing the model.\n",
    "\n",
    "### Step 1: Download Dataset\n",
    "The dataset is downloaded from Google Drive using a specific link. The data is a ZIP file that contains audio recordings for male and female voices.\n",
    "\n",
    "### Step 2: Extract Dataset\n",
    "Once the ZIP file is downloaded, it's extracted into a working directory to prepare the files for processing. The extracted folder contains subfolders for male and female audio recordings.\n",
    "\n",
    "### Step 3: Load and Prepare Audio Files\n",
    "Audio files are loaded from the extracted folder, and each audio file is labeled based on its folder (either \"male\" or \"female\"). These labels are converted into numerical values (e.g., 0 for male, 1 for female).\n",
    "\n",
    "### Step 4: Preprocess Audio Files\n",
    "Audio files are processed by resampling them to 16kHz if needed (to match the model's requirements). This step prepares the audio data for input into the Wav2Vec2 model.\n",
    "\n",
    "### Step 5: Split the Dataset\n",
    "The dataset is split into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate its performance.\n",
    "\n",
    "### Step 6: Train the Model\n",
    "The Wav2Vec2 model is fine-tuned using the preprocessed audio data. During training, the model learns to classify gender based on the audio recordings. Key metrics such as accuracy are monitored to ensure the model is performing well.\n",
    "\n",
    "### Step 7: Evaluate the Model\n",
    "After training, the model is evaluated using the test dataset. This step checks how accurately the model can classify new, unseen audio recordings into male or female categories.\n",
    "\n",
    "### Step 8: Save the Model\n",
    "Once the model is trained and evaluated, it is saved along with its processor. The model can now be used to predict the gender of new audio recordings.\n",
    "\n",
    "### Step 9: Make Predictions\n",
    "The saved model is used to predict gender from new audio files. The input audio is preprocessed similarly to the training data, and the model outputs whether the voice is male or female.\n",
    "\n",
    "### Step 10: Package and Share the Model\n",
    "Finally, the trained model is packaged into a ZIP file, making it easy to share or deploy elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-27T18:13:07.483416Z",
     "iopub.status.busy": "2024-09-27T18:13:07.482481Z",
     "iopub.status.idle": "2024-09-27T18:13:20.497518Z",
     "shell.execute_reply": "2024-09-27T18:13:20.496274Z",
     "shell.execute_reply.started": "2024-09-27T18:13:07.483359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T18:20:42.210270Z",
     "iopub.status.busy": "2024-09-27T18:20:42.209822Z",
     "iopub.status.idle": "2024-09-27T18:21:28.363779Z",
     "shell.execute_reply": "2024-09-27T18:21:28.362730Z",
     "shell.execute_reply.started": "2024-09-27T18:20:42.210232Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1b2NV1yM0u9bv9B6SmbCRa1u8Tl-N7FUn\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1b2NV1yM0u9bv9B6SmbCRa1u8Tl-N7FUn&confirm=t&uuid=e266c698-7751-4375-9f1a-d3bd79eae07d\n",
      "To: /kaggle/working/my_dataset.zip\n",
      "100%|██████████| 3.20G/3.20G [00:42<00:00, 75.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my_dataset.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# Correct link with direct download format\n",
    "file_id = '1b2NV1yM0u9bv9B6SmbCRa1u8Tl-N7FUn'  # Example: '1ABC123defGHI456jklMNO'\n",
    "gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'my_dataset.zip', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T18:23:27.515633Z",
     "iopub.status.busy": "2024-09-27T18:23:27.514455Z",
     "iopub.status.idle": "2024-09-27T18:23:58.673576Z",
     "shell.execute_reply": "2024-09-27T18:23:58.672457Z",
     "shell.execute_reply.started": "2024-09-27T18:23:27.515559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents extracted to /kaggle/working/unzipped_folder.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file_path = 'my_dataset.zip'\n",
    "output_folder = '/kaggle/working/unzipped_folder'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_folder)\n",
    "\n",
    "print(f\"Contents extracted to {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T18:26:56.516523Z",
     "iopub.status.busy": "2024-09-27T18:26:56.516072Z",
     "iopub.status.idle": "2024-09-27T20:13:50.391491Z",
     "shell.execute_reply": "2024-09-27T20:13:50.390465Z",
     "shell.execute_reply.started": "2024-09-27T18:26:56.516480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bc27f581c94822a92b53adcc1f4e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e77e1639ee4d91baa796ecd0fdae74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001b3b276ed743c4ba4a09879e5c1d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2615079cfe48457c92c594560bfdf0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321574ee64f2464283aae6cd5d3a9bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a572be88424a70b98b83216757be6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba6881bbf95450fb2c1f68e2df1e5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff354f1a541c45d383af6a5fafca0b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc073e47a74a496398533fb181d5137a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfed48ca506340b48294346e10db9bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42419ac382da4818a48692a901156864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240927_183325-a3kqjce6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mostafa-maroof/huggingface/runs/a3kqjce6' target=\"_blank\">./model_output</a></strong> to <a href='https://wandb.ai/mostafa-maroof/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mostafa-maroof/huggingface' target=\"_blank\">https://wandb.ai/mostafa-maroof/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mostafa-maroof/huggingface/runs/a3kqjce6' target=\"_blank\">https://wandb.ai/mostafa-maroof/huggingface/runs/a3kqjce6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1347' max='1347' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1347/1347 1:35:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.320929</td>\n",
       "      <td>0.901142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.220329</td>\n",
       "      <td>0.931774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181800</td>\n",
       "      <td>0.199163</td>\n",
       "      <td>0.939850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113/113 04:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.199162557721138, 'eval_accuracy': 0.9398496240601504, 'eval_runtime': 286.0688, 'eval_samples_per_second': 12.553, 'eval_steps_per_second': 0.395, 'epoch': 3.0}\n",
      "Model saved to ./model_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import evaluate  \n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"  \n",
    "num_labels = 2  # Male and Female\n",
    "output_dir = \"./model_output\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "def load_audio_files(data_dir):\n",
    "    dataset = []\n",
    "    for label in [\"male\", \"female\"]:\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        audio_files = [f for f in os.listdir(label_dir) if f.endswith('.wav')]\n",
    "        for audio_file in audio_files:\n",
    "            dataset.append({\"audio\": os.path.join(label_dir, audio_file), \"label\": label})\n",
    "    return dataset\n",
    "\n",
    "data_dir = \"/kaggle/working/unzipped_folder/Final Data\"\n",
    "audio_data = load_audio_files(data_dir)\n",
    "dataset = Dataset.from_list(audio_data)\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "def encode_labels(examples):\n",
    "    examples['label'] = 1 if examples['label'] == 'female' else 0\n",
    "    return examples\n",
    "\n",
    "# Apply encoding to the train and test datasets\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "test_dataset = test_dataset.map(encode_labels)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio, sr = torchaudio.load(examples[\"audio\"])\n",
    "\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        audio = resampler(audio)\n",
    "\n",
    "    audio = audio[0]  # Get the waveform (single channel)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Squeeze the input_values tensor to remove the extra dimensions\n",
    "    inputs['input_values'] = inputs['input_values'].squeeze(0)\n",
    "\n",
    "    # Add the label to the dictionary\n",
    "    inputs['label'] = examples['label']\n",
    "\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function)\n",
    "test_dataset = test_dataset.map(preprocess_function)\n",
    "\n",
    "# Remove the \"audio\" column (no longer needed after preprocessing)\n",
    "train_dataset = train_dataset.remove_columns([\"audio\"])\n",
    "test_dataset = test_dataset.remove_columns([\"audio\"])\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Data collator for padding\n",
    "def data_collator(features):\n",
    "    # Extract input values and labels\n",
    "    input_values = [f[\"input_values\"] for f in features]\n",
    "    labels = [f[\"label\"] for f in features]\n",
    "\n",
    "    # Pad input values\n",
    "    batch = processor.pad(\n",
    "        {\"input_values\": input_values},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Add labels to batch\n",
    "    batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return batch\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")  # Load accuracy metric\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,  # Add eval batch size\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after every epoch\n",
    "    save_steps=500,\n",
    "    learning_rate=5e-5,  # Adjusted Learning Rate\n",
    "    weight_decay=0.01,   # Regularization with weight decay\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,  \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", evaluation_results)\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "processor.save_pretrained(output_dir)  # Save the processor as well\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T20:14:58.556367Z",
     "iopub.status.busy": "2024-09-27T20:14:58.555442Z",
     "iopub.status.idle": "2024-09-27T20:18:29.329772Z",
     "shell.execute_reply": "2024-09-27T20:18:29.328794Z",
     "shell.execute_reply.started": "2024-09-27T20:14:58.556324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder zipped successfully: /kaggle/working/model_output.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "folder_to_zip = '/kaggle/working/model_output'  # Update this with your folder path\n",
    "\n",
    "zip_file_name = '/kaggle/working/model_output.zip'\n",
    "\n",
    "shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', folder_to_zip)\n",
    "\n",
    "print(f\"Folder zipped successfully: {zip_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T20:35:24.398340Z",
     "iopub.status.busy": "2024-09-27T20:35:24.397431Z",
     "iopub.status.idle": "2024-09-27T20:35:25.149447Z",
     "shell.execute_reply": "2024-09-27T20:35:25.148433Z",
     "shell.execute_reply.started": "2024-09-27T20:35:24.398295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gender: Female\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "model_dir = \"/kaggle/working/model_output\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_dir)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_dir)\n",
    "\n",
    "\n",
    "def predict_gender(audio_path):\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    if sampling_rate != 16000:  # assuming Wav2Vec2 expects 16kHz\n",
    "        resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "    \n",
    "    # Ensure the input shape is [batch_size, sequence_length]\n",
    "    inputs = processor(speech_array.squeeze(0), return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"Male\" if predicted_label == 0 else \"Female\"\n",
    "\n",
    "audio_path = \"/kaggle/input/gender-test/test/fe/74_combined.wav\"  # Replace this with the path to your audio file\n",
    "predicted_gender = predict_gender(audio_path)\n",
    "print(f\"Predicted Gender: {predicted_gender}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 198204582,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
